{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# generic space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nni.retiarii.nn.pytorch as nn\n",
    "import torch\n",
    "from collections import OrderedDict\n",
    "\n",
    "from nni import trace\n",
    "from nni.retiarii import model_wrapper\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "@model_wrapper\n",
    "class SimpleAutoencoderSpace(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(1, 16, 3, stride=2, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(16, 32, 3, stride=2, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, 3, stride=2, padding=1),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.ConvTranspose2d(64, 32, 3, stride=2, padding=1, output_padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(32, 16, 3, stride=2, padding=1, output_padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(16, 1, 3, stride=2, padding=1, output_padding=1),\n",
    "            nn.Sigmoid(),\n",
    "        )\n",
    "\n",
    "        out_layers = OrderedDict(\n",
    "            [\n",
    "            (\"RelU\", nn.Sequential(\n",
    "                nn.ConvTranspose2d(1, 1, 3, stride=2, padding=1, output_padding=1),\n",
    "                nn.ReLU())),\n",
    "            \n",
    "            (\"Sigmoid\", nn.Sequential(\n",
    "                nn.ConvTranspose2d(1, 1, 3, stride=2, padding=1, output_padding=1),\n",
    "                nn.Sigmoid())),\n",
    "            (\"SiLU\",  nn.Sequential(\n",
    "                nn.ConvTranspose2d(1, 1, 3, stride=2, padding=1, output_padding=1),\n",
    "                nn.SiLU())),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        self.out = nn.LayerChoice(out_layers,label='out')\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        x = self.decoder(x)\n",
    "        x = self.out(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# multi-trial search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Public\\Public_envs\\pub_ml_env\\Lib\\site-packages\\lightning\\fabric\\__init__.py:36: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('lightning.fabric')`.\n",
      "Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages\n",
      "  __import__(\"pkg_resources\").declare_namespace(__name__)\n",
      "c:\\Users\\Public\\Public_envs\\pub_ml_env\\Lib\\site-packages\\pkg_resources\\__init__.py:2350: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('lightning')`.\n",
      "Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages\n",
      "  declare_namespace(parent)\n",
      "c:\\Users\\Public\\Public_envs\\pub_ml_env\\Lib\\site-packages\\lightning\\pytorch\\__init__.py:36: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('lightning.pytorch')`.\n",
      "Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages\n",
      "  __import__(\"pkg_resources\").declare_namespace(__name__)\n",
      "c:\\Users\\Public\\Public_envs\\pub_ml_env\\Lib\\site-packages\\pkg_resources\\__init__.py:2350: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('lightning')`.\n",
      "Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages\n",
      "  declare_namespace(parent)\n"
     ]
    }
   ],
   "source": [
    "from nni.experiment import Experiment\n",
    "from nni.retiarii.evaluator import FunctionalEvaluator\n",
    "from nni.retiarii.experiment.pytorch import RetiariiExperiment, RetiariiExeConfig\n",
    "from nni.retiarii.strategy import DARTS, Random\n",
    "\n",
    "from darts.eval_lightning import LightningEval\n",
    "from darts.phantom import generate_phantom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-08-17 21:04:35] \u001b[32mCreating experiment, Experiment ID: \u001b[36mhvf5x6mo\u001b[0m\n",
      "[2023-08-17 21:04:35] \u001b[32mStarting web server...\u001b[0m\n",
      "[2023-08-17 21:04:36] \u001b[32mSetting up...\u001b[0m\n",
      "[2023-08-17 21:04:36] \u001b[32mWeb portal URLs: \u001b[36mhttp://169.254.138.100:8081 http://169.254.67.161:8081 http://169.254.50.13:8081 http://192.168.0.15:8081 http://127.0.0.1:8081\u001b[0m\n",
      "[2023-08-17 21:04:36] \u001b[32mDispatcher started\u001b[0m\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'LightningEval' object has no attribute '_dump'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 35\u001b[0m\n\u001b[0;32m     32\u001b[0m exp_config\u001b[39m.\u001b[39mtraining_service\u001b[39m.\u001b[39muse_active_gpu \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[0;32m     34\u001b[0m \u001b[39m# Execute\u001b[39;00m\n\u001b[1;32m---> 35\u001b[0m exp\u001b[39m.\u001b[39;49mrun(exp_config, \u001b[39m8081\u001b[39;49m)\n",
      "File \u001b[1;32mc:\\Users\\Public\\Public_envs\\pub_ml_env\\Lib\\site-packages\\nni\\nas\\experiment\\pytorch.py:298\u001b[0m, in \u001b[0;36mRetiariiExperiment.run\u001b[1;34m(self, config, port, debug)\u001b[0m\n\u001b[0;32m    291\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_action \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39mcreate\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[0;32m    292\u001b[0m     base_model_ir, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mapplied_mutators \u001b[39m=\u001b[39m preprocess_model(\n\u001b[0;32m    293\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbase_model, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mevaluator, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mapplied_mutators,\n\u001b[0;32m    294\u001b[0m         full_ir\u001b[39m=\u001b[39m\u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(canoni_conf\u001b[39m.\u001b[39mexecution_engine, (PyEngineConfig, BenchmarkEngineConfig)),\n\u001b[0;32m    295\u001b[0m         dummy_input\u001b[39m=\u001b[39mcanoni_conf\u001b[39m.\u001b[39mexecution_engine\u001b[39m.\u001b[39mdummy_input\n\u001b[0;32m    296\u001b[0m             \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(canoni_conf\u001b[39m.\u001b[39mexecution_engine, (BaseEngineConfig, CgoEngineConfig)) \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    297\u001b[0m     )\n\u001b[1;32m--> 298\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_save_experiment_checkpoint(base_model_ir, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mapplied_mutators, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstrategy,\n\u001b[0;32m    299\u001b[0m                                      canoni_conf\u001b[39m.\u001b[39;49mexperiment_working_directory)\n\u001b[0;32m    300\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_action \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39mresume\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[0;32m    301\u001b[0m     base_model_ir, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mapplied_mutators, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstrategy \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_load_experiment_checkpoint(\n\u001b[0;32m    302\u001b[0m         canoni_conf\u001b[39m.\u001b[39mexperiment_working_directory)\n",
      "File \u001b[1;32mc:\\Users\\Public\\Public_envs\\pub_ml_env\\Lib\\site-packages\\nni\\nas\\experiment\\pytorch.py:226\u001b[0m, in \u001b[0;36mRetiariiExperiment._save_experiment_checkpoint\u001b[1;34m(self, base_model_ir, applied_mutators, strategy, exp_work_dir)\u001b[0m\n\u001b[0;32m    224\u001b[0m ckp_path \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(exp_work_dir, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mid, \u001b[39m'\u001b[39m\u001b[39mcheckpoint\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m    225\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39m(os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(ckp_path, \u001b[39m'\u001b[39m\u001b[39mnas_model\u001b[39m\u001b[39m'\u001b[39m), \u001b[39m'\u001b[39m\u001b[39mw\u001b[39m\u001b[39m'\u001b[39m) \u001b[39mas\u001b[39;00m fp:\n\u001b[1;32m--> 226\u001b[0m     dump(base_model_ir\u001b[39m.\u001b[39;49m_dump(), fp, pickle_size_limit\u001b[39m=\u001b[39m\u001b[39mint\u001b[39m(os\u001b[39m.\u001b[39mgetenv(\u001b[39m'\u001b[39m\u001b[39mPICKLE_SIZE_LIMIT\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m64\u001b[39m \u001b[39m*\u001b[39m \u001b[39m1024\u001b[39m)))\n\u001b[0;32m    227\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39m(os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(ckp_path, \u001b[39m'\u001b[39m\u001b[39mapplied_mutators\u001b[39m\u001b[39m'\u001b[39m), \u001b[39m'\u001b[39m\u001b[39mw\u001b[39m\u001b[39m'\u001b[39m) \u001b[39mas\u001b[39;00m fp:\n\u001b[0;32m    228\u001b[0m     dump(applied_mutators, fp)\n",
      "File \u001b[1;32mc:\\Users\\Public\\Public_envs\\pub_ml_env\\Lib\\site-packages\\nni\\nas\\execution\\common\\graph.py:144\u001b[0m, in \u001b[0;36mModel._dump\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    142\u001b[0m ret[\u001b[39m'\u001b[39m\u001b[39mpython_init_params\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpython_init_params\n\u001b[0;32m    143\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mevaluator \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> 144\u001b[0m     ret[\u001b[39m'\u001b[39m\u001b[39m_evaluator\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mevaluator\u001b[39m.\u001b[39;49m_dump()\n\u001b[0;32m    145\u001b[0m \u001b[39mreturn\u001b[39;00m ret\n",
      "File \u001b[1;32mc:\\Users\\Public\\Public_envs\\pub_ml_env\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1614\u001b[0m, in \u001b[0;36mModule.__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   1612\u001b[0m     \u001b[39mif\u001b[39;00m name \u001b[39min\u001b[39;00m modules:\n\u001b[0;32m   1613\u001b[0m         \u001b[39mreturn\u001b[39;00m modules[name]\n\u001b[1;32m-> 1614\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mAttributeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39m'\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m object has no attribute \u001b[39m\u001b[39m'\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(\n\u001b[0;32m   1615\u001b[0m     \u001b[39mtype\u001b[39m(\u001b[39mself\u001b[39m)\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m, name))\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'LightningEval' object has no attribute '_dump'"
     ]
    }
   ],
   "source": [
    "# input image (phantom)\n",
    "resolution = 7\n",
    "phantom = generate_phantom(resolution=resolution)\n",
    "\n",
    "# search space\n",
    "model_space = SimpleAutoencoderSpace()\n",
    "\n",
    "evaluator = LightningEval(\n",
    "                model_space,\n",
    "                buffer_size=100,\n",
    "                num_iter=50,\n",
    "                lr=.0005,\n",
    "                noise_type='gaussian',\n",
    "                noise_factor=0.15,\n",
    "                resolution=resolution,\n",
    "                )\n",
    "\n",
    "# search strategy\n",
    "search_strategy = Random(dedup=True)\n",
    "\n",
    "# experiment\n",
    "exp = RetiariiExperiment(model_space, evaluator, [], search_strategy)\n",
    "exp_config = RetiariiExeConfig('local')\n",
    "exp_config.experiment_name = 'mnist_search'\n",
    "exp_config.trial_code_directory = 'C:/Users/Public/Public_VS_Code/NAS_test'\n",
    "exp_config.experiment_working_directory = 'C:/Users/Public/nni-experiments'\n",
    "\n",
    "exp_config.max_trial_number = 12   # spawn 50 trials at most\n",
    "exp_config.trial_concurrency = 2  # will run two trials concurrently\n",
    "\n",
    "exp_config.trial_gpu_number = 1 # will run 1 trial(s) concurrently\n",
    "exp_config.training_service.use_active_gpu = True\n",
    "\n",
    "# Execute\n",
    "exp.run(exp_config, 8081)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment = Experiment.connect(8081)\n",
    "experiment.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# one-shot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<nni.nas.oneshot.pytorch.strategy.DARTS at 0x14afce26390>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "from nni import trace\n",
    "from nni.retiarii import model_wrapper\n",
    "\n",
    "import nni.retiarii.nn.pytorch as nn\n",
    "\n",
    "from collections import OrderedDict\n",
    "from nni.experiment import Experiment\n",
    "from nni.retiarii.evaluator import FunctionalEvaluator\n",
    "from nni.retiarii.experiment.pytorch import RetiariiExperiment, RetiariiExeConfig\n",
    "from nni.retiarii.strategy import DARTS\n",
    "\n",
    "from darts.eval_lightning import LightningEval\n",
    "from darts.phantom import generate_phantom\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "@model_wrapper\n",
    "class SimpleAutoencoderSpace(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(1, 16, 3, stride=2, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(16, 32, 3, stride=2, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, 3, stride=2, padding=1),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.ConvTranspose2d(64, 32, 3, stride=2, padding=1, output_padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(32, 16, 3, stride=2, padding=1, output_padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(16, 1, 3, stride=2, padding=1, output_padding=1),\n",
    "            nn.Sigmoid(),\n",
    "        )\n",
    "\n",
    "        out_layers = OrderedDict(\n",
    "            [\n",
    "            (\"RelU\", nn.Sequential(\n",
    "                nn.ConvTranspose2d(1, 1, 3, stride=2, padding=1, output_padding=1),\n",
    "                nn.ReLU())),\n",
    "            \n",
    "            (\"Sigmoid\", nn.Sequential(\n",
    "                nn.ConvTranspose2d(1, 1, 3, stride=2, padding=1, output_padding=1),\n",
    "                nn.Sigmoid())),\n",
    "            (\"SiLU\",  nn.Sequential(\n",
    "                nn.ConvTranspose2d(1, 1, 3, stride=2, padding=1, output_padding=1),\n",
    "                nn.SiLU())),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        self.out = nn.LayerChoice(out_layers,label='out')\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        x = self.decoder(x)\n",
    "        x = self.out(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in C:\\Users\\thesis/.cache\\torch\\hub\\mateuszbuda_brain-segmentation-pytorch_master\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-08-17 18:03:14] \u001b[32mCreating experiment, Experiment ID: \u001b[36mkdp4nxmt\u001b[0m\n",
      "[2023-08-17 18:03:14] \u001b[32mStarting web server...\u001b[0m\n",
      "[2023-08-17 18:03:15] \u001b[32mSetting up...\u001b[0m\n",
      "[2023-08-17 18:03:15] \u001b[32mWeb portal URLs: \u001b[36mhttp://169.254.138.100:8081 http://169.254.67.161:8081 http://169.254.50.13:8081 http://10.0.0.172:8081 http://127.0.0.1:8081\u001b[0m\n",
      "[2023-08-17 18:03:15] \u001b[32mDispatcher started\u001b[0m\n"
     ]
    },
    {
     "ename": "PayloadTooLarge",
     "evalue": "Pickle too large when trying to dump LightningEval(\n  (net): SimpleAutoencoderSpace(\n    (encoder): Sequential(\n      (0): Conv2d(1, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n      (1): ReLU()\n      (2): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n      (3): ReLU()\n      (4): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n      (5): ReLU()\n    )\n    (decoder): Sequential(\n      (0): ConvTranspose2d(64, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n      (1): ReLU()\n      (2): ConvTranspose2d(32, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n      (3): ReLU()\n      (4): ConvTranspose2d(16, 1, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n      (5): Sigmoid()\n    )\n    (out): LayerChoice(OrderedDict([('RelU', Sequential(\n      (0): ConvTranspose2d(1, 1, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n      (1): ReLU()\n    )), ('Sigmoid', Sequential(\n      (0): ConvTranspose2d(1, 1, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n      (1): Sigmoid()\n    )), ('SiLU', Sequential(\n      (0): ConvTranspose2d(1, 1, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n      (1): SiLU()\n    ))]), label='out')\n  )\n  (criterion): MSELoss()\n). This might be caused by classes that are not decorated by @nni.trace. Another option is to force bytes pickling and try to raise pickle_size_limit.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPayloadTooLarge\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 42\u001b[0m\n\u001b[0;32m     39\u001b[0m exp_config\u001b[39m.\u001b[39mtraining_service\u001b[39m.\u001b[39muse_active_gpu \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[0;32m     41\u001b[0m \u001b[39m# Execute\u001b[39;00m\n\u001b[1;32m---> 42\u001b[0m exp\u001b[39m.\u001b[39;49mrun(exp_config, \u001b[39m8081\u001b[39;49m)\n\u001b[0;32m     45\u001b[0m exported_arch \u001b[39m=\u001b[39m experiment\u001b[39m.\u001b[39mexport_top_models(formatter\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mdict\u001b[39m\u001b[39m'\u001b[39m)[\u001b[39m0\u001b[39m:\u001b[39m10\u001b[39m]\n\u001b[0;32m     46\u001b[0m exported_arch[\u001b[39m0\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\Public\\Public_envs\\pub_ml_env\\Lib\\site-packages\\nni\\nas\\experiment\\pytorch.py:298\u001b[0m, in \u001b[0;36mRetiariiExperiment.run\u001b[1;34m(self, config, port, debug)\u001b[0m\n\u001b[0;32m    291\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_action \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39mcreate\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[0;32m    292\u001b[0m     base_model_ir, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mapplied_mutators \u001b[39m=\u001b[39m preprocess_model(\n\u001b[0;32m    293\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbase_model, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mevaluator, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mapplied_mutators,\n\u001b[0;32m    294\u001b[0m         full_ir\u001b[39m=\u001b[39m\u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(canoni_conf\u001b[39m.\u001b[39mexecution_engine, (PyEngineConfig, BenchmarkEngineConfig)),\n\u001b[0;32m    295\u001b[0m         dummy_input\u001b[39m=\u001b[39mcanoni_conf\u001b[39m.\u001b[39mexecution_engine\u001b[39m.\u001b[39mdummy_input\n\u001b[0;32m    296\u001b[0m             \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(canoni_conf\u001b[39m.\u001b[39mexecution_engine, (BaseEngineConfig, CgoEngineConfig)) \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    297\u001b[0m     )\n\u001b[1;32m--> 298\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_save_experiment_checkpoint(base_model_ir, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mapplied_mutators, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstrategy,\n\u001b[0;32m    299\u001b[0m                                      canoni_conf\u001b[39m.\u001b[39;49mexperiment_working_directory)\n\u001b[0;32m    300\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_action \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39mresume\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[0;32m    301\u001b[0m     base_model_ir, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mapplied_mutators, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstrategy \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_load_experiment_checkpoint(\n\u001b[0;32m    302\u001b[0m         canoni_conf\u001b[39m.\u001b[39mexperiment_working_directory)\n",
      "File \u001b[1;32mc:\\Users\\Public\\Public_envs\\pub_ml_env\\Lib\\site-packages\\nni\\nas\\experiment\\pytorch.py:226\u001b[0m, in \u001b[0;36mRetiariiExperiment._save_experiment_checkpoint\u001b[1;34m(self, base_model_ir, applied_mutators, strategy, exp_work_dir)\u001b[0m\n\u001b[0;32m    224\u001b[0m ckp_path \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(exp_work_dir, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mid, \u001b[39m'\u001b[39m\u001b[39mcheckpoint\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m    225\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39m(os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(ckp_path, \u001b[39m'\u001b[39m\u001b[39mnas_model\u001b[39m\u001b[39m'\u001b[39m), \u001b[39m'\u001b[39m\u001b[39mw\u001b[39m\u001b[39m'\u001b[39m) \u001b[39mas\u001b[39;00m fp:\n\u001b[1;32m--> 226\u001b[0m     dump(base_model_ir\u001b[39m.\u001b[39;49m_dump(), fp, pickle_size_limit\u001b[39m=\u001b[39;49m\u001b[39mint\u001b[39;49m(os\u001b[39m.\u001b[39;49mgetenv(\u001b[39m'\u001b[39;49m\u001b[39mPICKLE_SIZE_LIMIT\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39m64\u001b[39;49m \u001b[39m*\u001b[39;49m \u001b[39m1024\u001b[39;49m)))\n\u001b[0;32m    227\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39m(os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(ckp_path, \u001b[39m'\u001b[39m\u001b[39mapplied_mutators\u001b[39m\u001b[39m'\u001b[39m), \u001b[39m'\u001b[39m\u001b[39mw\u001b[39m\u001b[39m'\u001b[39m) \u001b[39mas\u001b[39;00m fp:\n\u001b[0;32m    228\u001b[0m     dump(applied_mutators, fp)\n",
      "File \u001b[1;32mc:\\Users\\Public\\Public_envs\\pub_ml_env\\Lib\\site-packages\\nni\\common\\serializer.py:341\u001b[0m, in \u001b[0;36mdump\u001b[1;34m(obj, fp, use_trace, pickle_size_limit, allow_nan, **json_tricks_kwargs)\u001b[0m\n\u001b[0;32m    339\u001b[0m \u001b[39mif\u001b[39;00m json_tricks_kwargs\u001b[39m.\u001b[39mget(\u001b[39m'\u001b[39m\u001b[39mcompression\u001b[39m\u001b[39m'\u001b[39m) \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    340\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m'\u001b[39m\u001b[39mIf you meant to compress the dumped payload, please use `dump_bytes`.\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m--> 341\u001b[0m result \u001b[39m=\u001b[39m _dump(\n\u001b[0;32m    342\u001b[0m     obj\u001b[39m=\u001b[39;49mobj,\n\u001b[0;32m    343\u001b[0m     fp\u001b[39m=\u001b[39;49mfp,\n\u001b[0;32m    344\u001b[0m     use_trace\u001b[39m=\u001b[39;49muse_trace,\n\u001b[0;32m    345\u001b[0m     pickle_size_limit\u001b[39m=\u001b[39;49mpickle_size_limit,\n\u001b[0;32m    346\u001b[0m     allow_nan\u001b[39m=\u001b[39;49mallow_nan,\n\u001b[0;32m    347\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mjson_tricks_kwargs)\n\u001b[0;32m    348\u001b[0m \u001b[39mreturn\u001b[39;00m cast(\u001b[39mstr\u001b[39m, result)\n",
      "File \u001b[1;32mc:\\Users\\Public\\Public_envs\\pub_ml_env\\Lib\\site-packages\\nni\\common\\serializer.py:390\u001b[0m, in \u001b[0;36m_dump\u001b[1;34m(obj, fp, use_trace, pickle_size_limit, allow_nan, **json_tricks_kwargs)\u001b[0m\n\u001b[0;32m    387\u001b[0m json_tricks_kwargs[\u001b[39m'\u001b[39m\u001b[39mallow_nan\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m allow_nan\n\u001b[0;32m    389\u001b[0m \u001b[39mif\u001b[39;00m fp \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> 390\u001b[0m     \u001b[39mreturn\u001b[39;00m json_tricks\u001b[39m.\u001b[39;49mdump(obj, fp, obj_encoders\u001b[39m=\u001b[39;49mencoders, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mjson_tricks_kwargs)\n\u001b[0;32m    391\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    392\u001b[0m     \u001b[39mreturn\u001b[39;00m json_tricks\u001b[39m.\u001b[39mdumps(obj, obj_encoders\u001b[39m=\u001b[39mencoders, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mjson_tricks_kwargs)\n",
      "File \u001b[1;32mc:\\Users\\Public\\Public_envs\\pub_ml_env\\Lib\\site-packages\\json_tricks\\nonp.py:135\u001b[0m, in \u001b[0;36mdump\u001b[1;34m(obj, fp, sort_keys, cls, obj_encoders, extra_obj_encoders, primitives, compression, force_flush, allow_nan, conv_str_byte, fallback_encoders, properties, **jsonkwargs)\u001b[0m\n\u001b[0;32m    133\u001b[0m \u001b[39mif\u001b[39;00m (\u001b[39misinstance\u001b[39m(obj, str_type) \u001b[39mor\u001b[39;00m \u001b[39mhasattr\u001b[39m(obj, \u001b[39m'\u001b[39m\u001b[39mwrite\u001b[39m\u001b[39m'\u001b[39m)) \u001b[39mand\u001b[39;00m \u001b[39misinstance\u001b[39m(fp, (\u001b[39mlist\u001b[39m, \u001b[39mdict\u001b[39m)):\n\u001b[0;32m    134\u001b[0m \t\u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m'\u001b[39m\u001b[39mjson-tricks dump arguments are in the wrong order: provide the data to be serialized before file handle\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m--> 135\u001b[0m txt \u001b[39m=\u001b[39m dumps(obj, sort_keys\u001b[39m=\u001b[39;49msort_keys, \u001b[39mcls\u001b[39;49m\u001b[39m=\u001b[39;49m\u001b[39mcls\u001b[39;49m, obj_encoders\u001b[39m=\u001b[39;49mobj_encoders, extra_obj_encoders\u001b[39m=\u001b[39;49mextra_obj_encoders,\n\u001b[0;32m    136\u001b[0m \tprimitives\u001b[39m=\u001b[39;49mprimitives, compression\u001b[39m=\u001b[39;49mcompression, allow_nan\u001b[39m=\u001b[39;49mallow_nan, conv_str_byte\u001b[39m=\u001b[39;49mconv_str_byte,\n\u001b[0;32m    137\u001b[0m \tfallback_encoders\u001b[39m=\u001b[39;49mfallback_encoders, properties\u001b[39m=\u001b[39;49mproperties, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mjsonkwargs)\n\u001b[0;32m    138\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(fp, str_type):\n\u001b[0;32m    139\u001b[0m \t\u001b[39mif\u001b[39;00m compression:\n",
      "File \u001b[1;32mc:\\Users\\Public\\Public_envs\\pub_ml_env\\Lib\\site-packages\\json_tricks\\nonp.py:109\u001b[0m, in \u001b[0;36mdumps\u001b[1;34m(obj, sort_keys, cls, obj_encoders, extra_obj_encoders, primitives, compression, allow_nan, conv_str_byte, fallback_encoders, properties, **jsonkwargs)\u001b[0m\n\u001b[0;32m    105\u001b[0m \t\u001b[39mcls\u001b[39m \u001b[39m=\u001b[39m TricksEncoder\n\u001b[0;32m    106\u001b[0m combined_encoder \u001b[39m=\u001b[39m \u001b[39mcls\u001b[39m(sort_keys\u001b[39m=\u001b[39msort_keys, obj_encoders\u001b[39m=\u001b[39mencoders, allow_nan\u001b[39m=\u001b[39mallow_nan,\n\u001b[0;32m    107\u001b[0m \tprimitives\u001b[39m=\u001b[39mprimitives, fallback_encoders\u001b[39m=\u001b[39mfallback_encoders,\n\u001b[0;32m    108\u001b[0m   \tproperties\u001b[39m=\u001b[39mproperties, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mjsonkwargs)\n\u001b[1;32m--> 109\u001b[0m txt \u001b[39m=\u001b[39m combined_encoder\u001b[39m.\u001b[39;49mencode(obj)\n\u001b[0;32m    110\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m is_py3 \u001b[39mand\u001b[39;00m \u001b[39misinstance\u001b[39m(txt, \u001b[39mstr\u001b[39m):\n\u001b[0;32m    111\u001b[0m \ttxt \u001b[39m=\u001b[39m unicode(txt, ENCODING)\n",
      "File \u001b[1;32mC:\\Python311\\Lib\\json\\encoder.py:200\u001b[0m, in \u001b[0;36mJSONEncoder.encode\u001b[1;34m(self, o)\u001b[0m\n\u001b[0;32m    196\u001b[0m         \u001b[39mreturn\u001b[39;00m encode_basestring(o)\n\u001b[0;32m    197\u001b[0m \u001b[39m# This doesn't pass the iterator directly to ''.join() because the\u001b[39;00m\n\u001b[0;32m    198\u001b[0m \u001b[39m# exceptions aren't as detailed.  The list call should be roughly\u001b[39;00m\n\u001b[0;32m    199\u001b[0m \u001b[39m# equivalent to the PySequence_Fast that ''.join() would do.\u001b[39;00m\n\u001b[1;32m--> 200\u001b[0m chunks \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49miterencode(o, _one_shot\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[0;32m    201\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(chunks, (\u001b[39mlist\u001b[39m, \u001b[39mtuple\u001b[39m)):\n\u001b[0;32m    202\u001b[0m     chunks \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(chunks)\n",
      "File \u001b[1;32mC:\\Python311\\Lib\\json\\encoder.py:258\u001b[0m, in \u001b[0;36mJSONEncoder.iterencode\u001b[1;34m(self, o, _one_shot)\u001b[0m\n\u001b[0;32m    253\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    254\u001b[0m     _iterencode \u001b[39m=\u001b[39m _make_iterencode(\n\u001b[0;32m    255\u001b[0m         markers, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdefault, _encoder, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mindent, floatstr,\n\u001b[0;32m    256\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mkey_separator, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mitem_separator, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msort_keys,\n\u001b[0;32m    257\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mskipkeys, _one_shot)\n\u001b[1;32m--> 258\u001b[0m \u001b[39mreturn\u001b[39;00m _iterencode(o, \u001b[39m0\u001b[39;49m)\n",
      "File \u001b[1;32mc:\\Users\\Public\\Public_envs\\pub_ml_env\\Lib\\site-packages\\json_tricks\\encoders.py:77\u001b[0m, in \u001b[0;36mTricksEncoder.default\u001b[1;34m(self, obj, *args, **kwargs)\u001b[0m\n\u001b[0;32m     75\u001b[0m prev_id \u001b[39m=\u001b[39m \u001b[39mid\u001b[39m(obj)\n\u001b[0;32m     76\u001b[0m \u001b[39mfor\u001b[39;00m encoder \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mobj_encoders:\n\u001b[1;32m---> 77\u001b[0m \tobj \u001b[39m=\u001b[39m encoder(obj, primitives\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mprimitives, is_changed\u001b[39m=\u001b[39;49m\u001b[39mid\u001b[39;49m(obj) \u001b[39m!=\u001b[39;49m prev_id, properties\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mproperties)\n\u001b[0;32m     78\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mid\u001b[39m(obj) \u001b[39m==\u001b[39m prev_id:\n\u001b[0;32m     79\u001b[0m \t\u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m((\u001b[39m'\u001b[39m\u001b[39mObject of type \u001b[39m\u001b[39m{0:}\u001b[39;00m\u001b[39m could not be encoded by \u001b[39m\u001b[39m{1:}\u001b[39;00m\u001b[39m using encoders [\u001b[39m\u001b[39m{2:s}\u001b[39;00m\u001b[39m]. \u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m     80\u001b[0m \t\t\u001b[39m'\u001b[39m\u001b[39mYou can add an encoders for this type using `extra_obj_encoders`. If you want to \u001b[39m\u001b[39m\\'\u001b[39;00m\u001b[39mskip\u001b[39m\u001b[39m\\'\u001b[39;00m\u001b[39m this \u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m     81\u001b[0m \t\t\u001b[39m'\u001b[39m\u001b[39mobject, consider using `fallback_encoders` like `str` or `lambda o: None`.\u001b[39m\u001b[39m'\u001b[39m)\u001b[39m.\u001b[39mformat(\n\u001b[0;32m     82\u001b[0m \t\t\t\u001b[39mtype\u001b[39m(obj), \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m, \u001b[39m'\u001b[39m\u001b[39m, \u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mjoin(\u001b[39mstr\u001b[39m(encoder) \u001b[39mfor\u001b[39;00m encoder \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mobj_encoders)))\n",
      "File \u001b[1;32mc:\\Users\\Public\\Public_envs\\pub_ml_env\\Lib\\site-packages\\json_tricks\\utils.py:66\u001b[0m, in \u001b[0;36mfiltered_wrapper.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     65\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mwrapper\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m---> 66\u001b[0m \t\u001b[39mreturn\u001b[39;00m encoder(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49m{k: v \u001b[39mfor\u001b[39;49;00m k, v \u001b[39min\u001b[39;49;00m kwargs\u001b[39m.\u001b[39;49mitems() \u001b[39mif\u001b[39;49;00m k \u001b[39min\u001b[39;49;00m names})\n",
      "File \u001b[1;32mc:\\Users\\Public\\Public_envs\\pub_ml_env\\Lib\\site-packages\\nni\\common\\serializer.py:864\u001b[0m, in \u001b[0;36m_json_tricks_any_object_encode\u001b[1;34m(obj, primitives, pickle_size_limit)\u001b[0m\n\u001b[0;32m    862\u001b[0m b \u001b[39m=\u001b[39m cloudpickle\u001b[39m.\u001b[39mdumps(obj)\n\u001b[0;32m    863\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(b) \u001b[39m>\u001b[39m pickle_size_limit \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m--> 864\u001b[0m     \u001b[39mraise\u001b[39;00m PayloadTooLarge(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mPickle too large when trying to dump \u001b[39m\u001b[39m{\u001b[39;00mobj\u001b[39m}\u001b[39;00m\u001b[39m. This might be caused by classes that are \u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m    865\u001b[0m                           \u001b[39m'\u001b[39m\u001b[39mnot decorated by @nni.trace. Another option is to force bytes pickling and \u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m    866\u001b[0m                           \u001b[39m'\u001b[39m\u001b[39mtry to raise pickle_size_limit.\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m    867\u001b[0m \u001b[39m# use base64 to dump a bytes array\u001b[39;00m\n\u001b[0;32m    868\u001b[0m \u001b[39mreturn\u001b[39;00m {\n\u001b[0;32m    869\u001b[0m     \u001b[39m'\u001b[39m\u001b[39m__nni_obj__\u001b[39m\u001b[39m'\u001b[39m: base64\u001b[39m.\u001b[39mb64encode(b)\u001b[39m.\u001b[39mdecode()\n\u001b[0;32m    870\u001b[0m }\n",
      "\u001b[1;31mPayloadTooLarge\u001b[0m: Pickle too large when trying to dump LightningEval(\n  (net): SimpleAutoencoderSpace(\n    (encoder): Sequential(\n      (0): Conv2d(1, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n      (1): ReLU()\n      (2): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n      (3): ReLU()\n      (4): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n      (5): ReLU()\n    )\n    (decoder): Sequential(\n      (0): ConvTranspose2d(64, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n      (1): ReLU()\n      (2): ConvTranspose2d(32, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n      (3): ReLU()\n      (4): ConvTranspose2d(16, 1, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n      (5): Sigmoid()\n    )\n    (out): LayerChoice(OrderedDict([('RelU', Sequential(\n      (0): ConvTranspose2d(1, 1, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n      (1): ReLU()\n    )), ('Sigmoid', Sequential(\n      (0): ConvTranspose2d(1, 1, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n      (1): Sigmoid()\n    )), ('SiLU', Sequential(\n      (0): ConvTranspose2d(1, 1, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n      (1): SiLU()\n    ))]), label='out')\n  )\n  (criterion): MSELoss()\n). This might be caused by classes that are not decorated by @nni.trace. Another option is to force bytes pickling and try to raise pickle_size_limit."
     ]
    }
   ],
   "source": [
    "# input image (phantom)\n",
    "resolution = 7\n",
    "phantom = generate_phantom(resolution=resolution)\n",
    "\n",
    "# search space\n",
    "model_space = SimpleAutoencoderSpace()\n",
    "\n",
    "# model\n",
    "model = torch.hub.load('mateuszbuda/brain-segmentation-pytorch', 'unet', in_channels=1, out_channels=1, init_features=64, pretrained=False)\n",
    "\n",
    "\n",
    "# eval module\n",
    "module = LightningEval(\n",
    "                model_space, # not sure if space or model goes here\n",
    "                phantom=phantom, \n",
    "                buffer_size=100,\n",
    "                num_iter=50,\n",
    "                lr=0.00005, \n",
    "                noise_type='gaussian', \n",
    "                noise_factor=0.15, \n",
    "                resolution=resolution, \n",
    "                )\n",
    "evaluator = FunctionalEvaluator(module)\n",
    "\n",
    "# search strategy\n",
    "strategy = DARTS()\n",
    "\n",
    "# experiment\n",
    "exp = RetiariiExperiment(model_space, evaluator, [], strategy)\n",
    "exp_config = RetiariiExeConfig('local')\n",
    "exp_config.experiment_name = 'mnist_search'\n",
    "exp_config.trial_code_directory = 'C:/Users/Public/Public_VS_Code/NAS_test'\n",
    "exp_config.experiment_working_directory = 'C:/Users/Public/nni-experiments'\n",
    "\n",
    "exp_config.max_trial_number = 12   # spawn 50 trials at most\n",
    "exp_config.trial_concurrency = 2  # will run two trials concurrently\n",
    "\n",
    "exp_config.trial_gpu_number = 1 # will run 1 trial(s) concurrently\n",
    "exp_config.training_service.use_active_gpu = True\n",
    "\n",
    "# Execute\n",
    "exp.run(exp_config, 8081)\n",
    "\n",
    "\n",
    "exported_arch = experiment.export_top_models(formatter='dict')[0:10]\n",
    "exported_arch[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-08-17 18:03:33] \u001b[32mConnect to port 8081 success, experiment id is kdp4nxmt, status is RUNNING.\u001b[0m\n",
      "[2023-08-17 18:03:33] \u001b[32mStopping experiment, please wait...\u001b[0m\n",
      "[2023-08-17 18:03:33] \u001b[32mExperiment stopped\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-08-17 18:03:33] \u001b[32mDispatcher exiting...\u001b[0m\n",
      "[2023-08-17 18:03:36] \u001b[32mDispatcher terminiated\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "experiment = Experiment.connect(8081)\n",
    "experiment.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pub_ml_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
